{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e82dd9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Basic imports for data handling, training, and visualization\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load EfficientNet model from timm (PyTorch Image Models)\n",
    "import timm\n",
    "\n",
    "# Set device to GPU if available, else use CPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5818bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the new dataset directory for fine-tuning\n",
    "DATASET_DIR = 'Dataset-New'\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE    = 32         # Number of images per batch\n",
    "NUM_EPOCHS    = 5          # Number of fine-tuning epochs\n",
    "LEARNING_RATE = 1e-4       # Learning rate for optimizer\n",
    "IMG_SIZE      = 224        # Input image size for EfficientNet-B0\n",
    "\n",
    "# Proportions for splitting the dataset\n",
    "train_ratio = 0.8\n",
    "val_ratio   = 0.1\n",
    "test_ratio  = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e310cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training set\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),        # Resize to model input size\n",
    "    transforms.RandomHorizontalFlip(),              # Augment with horizontal flips\n",
    "    transforms.RandomRotation(10),                  # Augment with small rotations\n",
    "    transforms.ToTensor(),                          # Convert to PyTorch tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],      # Normalize using ImageNet means\n",
    "                         [0.229, 0.224, 0.225])      # and standard deviations\n",
    "])\n",
    "\n",
    "# Only resize and normalize validation and test sets (no augmentation)\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b7703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images  : 36000\n",
      "Train samples : 28800\n",
      "Val samples   : 3600\n",
      "Test samples  : 3600\n",
      "Classes: ['fake', 'real']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset containing 'fake' and 'real' subfolders\n",
    "full_dataset = datasets.ImageFolder(DATASET_DIR, transform=None)\n",
    "\n",
    "# Split dataset into train, validation, and test subsets\n",
    "num_samples = len(full_dataset)\n",
    "train_size  = int(train_ratio * num_samples)\n",
    "val_size    = int(val_ratio * num_samples)\n",
    "test_size   = num_samples - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Apply transforms to each split\n",
    "train_dataset.dataset.transform = train_transforms\n",
    "val_dataset.dataset.transform   = val_test_transforms\n",
    "test_dataset.dataset.transform  = val_test_transforms\n",
    "\n",
    "print(\"Classes:\", full_dataset.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99eb0de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b30d3a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model from: efficientnet_b0_real_vs_fake.pth\n"
     ]
    }
   ],
   "source": [
    "# Initialize EfficientNet-B0 model (no pretrained weights for fine-tuning)\n",
    "model = timm.create_model('efficientnet_b0', pretrained=False)\n",
    "\n",
    "# Modify the final classifier layer for binary classification (2 output classes)\n",
    "num_features = model.classifier.in_features\n",
    "model.classifier = nn.Linear(num_features, 2)\n",
    "\n",
    "# Load weights from previously trained model\n",
    "checkpoint_path = 'efficientnet_b0_real_vs_fake.pth'\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n",
    "\n",
    "# Move model to the appropriate device (GPU or CPU)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "print(\"Loaded pretrained model from:\", checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "537ba31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze all layers except the classifier.\n"
     ]
    }
   ],
   "source": [
    "# Freeze all layers except the classifier for partial fine-tuning\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "print(\"Froze all layers except the classifier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d60e7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function (cross-entropy for classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get parameters that require gradients (trainable parameters)\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# Initialize optimizer (Adam) for only the trainable parameters\n",
    "optimizer = optim.Adam(trainable_params, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb06da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device=DEVICE):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct      = 0\n",
    "    total        = 0\n",
    "    \n",
    "    # Iterate through the training data\n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss    = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagate the error\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)  # Accumulate loss\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
    "        correct      += (predicted == labels).sum().item()  # Count correct predictions\n",
    "        total        += labels.size(0)  # Total number of samples\n",
    "    \n",
    "    # Compute average loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / total if total else 0.0\n",
    "    epoch_acc  = correct / total if total else 0.0\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device=DEVICE):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct      = 0\n",
    "    total        = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        # Iterate through the validation data\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validating\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss    = criterion(outputs, labels)  # Compute loss\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)  # Accumulate loss\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
    "            correct      += (predicted == labels).sum().item()  # Count correct predictions\n",
    "            total        += labels.size(0)  # Total number of samples\n",
    "    \n",
    "    # Compute average loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / total if total else 0.0\n",
    "    epoch_acc  = correct / total if total else 0.0\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4533c317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n",
      "  Train Loss: 2.6258  | Train Acc: 0.5362\n",
      "  Val   Loss: 0.6697    | Val Acc:   0.5967\n",
      "==> Best model saved with val_acc = 0.5967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5]\n",
      "  Train Loss: 0.6512  | Train Acc: 0.6218\n",
      "  Val   Loss: 0.6491    | Val Acc:   0.6222\n",
      "==> Best model saved with val_acc = 0.6222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5]\n",
      "  Train Loss: 0.6356  | Train Acc: 0.6408\n",
      "  Val   Loss: 0.6395    | Val Acc:   0.6419\n",
      "==> Best model saved with val_acc = 0.6419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5]\n",
      "  Train Loss: 0.6243  | Train Acc: 0.6503\n",
      "  Val   Loss: 0.6318    | Val Acc:   0.6497\n",
      "==> Best model saved with val_acc = 0.6497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5]\n",
      "  Train Loss: 0.6163  | Train Acc: 0.6644\n",
      "  Val   Loss: 0.6271    | Val Acc:   0.6569\n",
      "==> Best model saved with val_acc = 0.6569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Track best validation accuracy for saving the best model\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# Main training loop for multiple epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train for one epoch and validate\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "    val_loss,   val_acc   = validate_one_epoch(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    # Print training and validation results\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}  | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f}    | Val Acc:   {val_acc:.4f}\")\n",
    "    \n",
    "    # Save model if validation accuracy improves\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"efficientnet_b0_finetuned_newdata.pth\")  # Save best model weights\n",
    "        print(f\"==> Best model saved with val_acc = {best_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3754c916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nisha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.5848 | Train Acc: 0.6909\n",
      "  Val   Loss: 0.5957 | Val Acc:   0.6897\n",
      "==> Best model saved with val_loss = 0.5957\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.5825 | Train Acc: 0.6904\n",
      "  Val   Loss: 0.6120 | Val Acc:   0.6881\n",
      "No improvement for 1 epoch(s).\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.5823 | Train Acc: 0.6944\n",
      "  Val   Loss: 0.5959 | Val Acc:   0.6889\n",
      "No improvement for 2 epoch(s).\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.5816 | Train Acc: 0.6923\n",
      "  Val   Loss: 0.5948 | Val Acc:   0.6900\n",
      "==> Best model saved with val_loss = 0.5948\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.5812 | Train Acc: 0.6937\n",
      "  Val   Loss: 0.5928 | Val Acc:   0.6944\n",
      "==> Best model saved with val_loss = 0.5928\n",
      "Resumed model set to best weights achieved during additional training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Additional Training with Learning Rate Scheduler and Early Stopping\n",
    "\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters for fine-tuning\n",
    "additional_epochs = 5  # Number of additional epochs you want to run\n",
    "patience = 3           # Number of epochs to wait for improvement before early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Define a learning rate scheduler that monitors validation loss\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.1, \n",
    "    patience=3, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Save the best model weights\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "for epoch in range(additional_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{additional_epochs}\")\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "    # Validation phase\n",
    "    val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "    \n",
    "    # Step the scheduler with the validation loss value\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Check if validation loss improved, save the model and reset patience counter\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"efficientnet_b0_finetuned_newdata_best.pth\")\n",
    "        print(f\"==> Best model saved with val_loss = {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement for {patience_counter} epoch(s).\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model weights for further evaluation or inference\n",
    "model.load_state_dict(best_model_wts)\n",
    "print(\"Resumed model set to best weights achieved during additional training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df373fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate reduced further for extra training.\n",
      "Extra Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.5761 | Train Acc: 0.6969\n",
      "  Val   Loss: 0.5920 | Val Acc:   0.6925\n",
      "==> Best model updated with val_loss = 0.5920\n",
      "Extra Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.5769 | Train Acc: 0.6947\n",
      "  Val   Loss: 0.5902 | Val Acc:   0.6939\n",
      "==> Best model updated with val_loss = 0.5902\n",
      "Extra Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.5758 | Train Acc: 0.6938\n",
      "  Val   Loss: 0.5897 | Val Acc:   0.6933\n",
      "==> Best model updated with val_loss = 0.5897\n",
      "Extra training complete. Model set to best weights from extra training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters for fine-tuning with early stopping and learning rate scheduler\n",
    "additional_epochs = 5  # Number of additional epochs for training\n",
    "patience = 3           # Patience for early stopping (epochs without improvement)\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Learning rate scheduler: reduces LR if validation loss plateaus\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.1,   # Reduce LR by a factor of 0.1\n",
    "    patience=3,   # Wait for 3 epochs without improvement\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Save the best model weights\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "for epoch in range(additional_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{additional_epochs}\")\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "    # Validation phase\n",
    "    val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "    \n",
    "    # Step the scheduler with the validation loss value\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save the best model weights and reset patience counter if loss improves\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"efficientnet_b0_finetuned_newdata_best.pth\")\n",
    "        print(f\"==> Best model saved with val_loss = {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement for {patience_counter} epoch(s).\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model weights after training is complete\n",
    "model.load_state_dict(best_model_wts)\n",
    "print(\"Resumed model set to best weights achieved during additional training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
